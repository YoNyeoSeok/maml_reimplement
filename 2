import torch
from torch import nn
from torch.nn import functional as F
import torch.optim as optim
#from torchviz import make_dot

from model import Simple
from data.data_gen import Sinewave_data
import numpy as np
from collections import OrderedDict
import argparse
import copy
import os
import random

from PIL import Image

parser = argparse.ArgumentParser()
parser.add_argument('--datasource', type=str, nargs='?', default="sinusoidal", choices=["sinusoidal", "omniglot"])
parser.add_argument('--batch_size', type=int, nargs='?', default=25)
parser.add_argument('--log_folder', type=str, default="omniglot5way")
parser.add_argument('--alpha', type=float, nargs='?', default=1e-2)
parser.add_argument('--beta', type=float, nargs='?', default=1e-2)
args = parser.parse_args()

device = "cuda:0" if torch.cuda.is_available() else "cpu"

from maml import Construct_Sequential, Change_view



class MAML(nn.Module):
    def __init__(self, model, num_task=1):
        super(MAML, self).__init__()

        self.model = model
        self.num_task = 1

    def forward(self, x):
        pass

    def construct_model(self, args):
        self.model = nn.Sequential(args.model)

def mse(predict, target):
    return torch.mean((predict-target)**2, axis=0)
    

def main():
    # Sinusoidal data
    if args.datasource is "sinusoidal":
        data_gen = Sinewave_data()
        inputs, targets = data_gen.gen(n=5, x_min=-5, x_max=5)
    # Omniglot data
    elif args.datasource is "omniglot":
        classification = True
        data_folder = './data/omniglot_resized'
        character_folders = [os.path.join(data_folder, family, character) \
                for family in os.listdir(data_folder) \
                if os.path.isdir(os.path.join(data_folder, family)) \
                for character in os.listdir(os.path.join(data_folder, family))]
        #print(np.array(character_folders).shape)
        random.seed(1)
        random.shuffle(character_folders)
        num_val = 100
        num_train = 1200 - num_val
    
        metatrain_character_folders = character_folders[:num_train]
        metaval_character_folders = character_folders[num_train:num_train+num_val]          # for test
        metaval_character_folders = character_folders[num_train+num_val:]                   # validation in training
        rotations = [0, 90, 180, 270]


    config = {}
    # model for Sinusoidal
    if args.datasource is "sinusoidal":
        config['model'] = OrderedDict([
            ('linear1', nn.Linear(1, 40)),
            ('relu1', nn.ReLU()),
            ('linear2', nn.Linear(40, 40)),
            ('relu2', nn.ReLU()),
            ('linear3', nn.Linear(40, 1))
            ])
        model = Construct_Sequential(config['model'])
    # model for Omniglot
    elif args.datasource is "omniglot":
        config['features'] = OrderedDict([
    		('conv1', nn.Conv2d(1, 64, (3, 3))),
    	    ('bn1', nn.BatchNorm2d(64, track_running_stats=False)),
    	    ('relu1', nn.ReLU()),
    	    ('pool1', nn.MaxPool2d((2, 2))),
    	    ('conv2', nn.Conv2d(64, 64, (3, 3))),
    	    ('bn2', nn.BatchNorm2d(64, track_running_stats=False)),
    	    ('relu2', nn.ReLU()),
    	    ('pool2', nn.MaxPool2d((2, 2))),
    	    ('conv3', nn.Conv2d(64, 64, (3, 3))),
    	    ('bn3', nn.BatchNorm2d(64, track_running_stats=False)),
    	    ('relu3', nn.ReLU()),
    	    ('pool3', nn.MaxPool2d((2, 2))),
    	    ])  
        config['view'] = OrderedDict([
            ('view', Change_view((-1, 64)))
            ])
        config['classifier'] = OrderedDict([
            ('fc1', nn.Linear(64, 5))
            ])
        config['model'] = OrderedDict(list(config['features'].items()) 
                + list(config['view'].items()) + list(config['classifier'].items()))
        model = Construct_Sequential(config['model']).to(device)
#    for name, param in model.named_parameters():
#        print(name, param.shape)
        for (name, module) in model._modules.items():
            print(name, module)
            if isinstance(module, nn.Conv2d):
            #if hasattr(module, 'weight') and not 'bn' in name:
                torch.nn.init.xavier_uniform_(module.weight)
                #print(module.weight)
            if isinstance(module, nn.Linear):
                torch.nn.init.normal_(module.weight)
                #print(module.weight)
    print(model)
#    print('model', torch.cuda.memory_allocated(torch.cuda.current_device()))
#    fast_net = Construct_Sequential(config['model']).to(device)
	
	# loss for Sinusoidal
    if args.datasource is "sinusoidal":
        loss_func = F.mse_loss 
        loss_func = torch.nn.MSELoss()
	# loss for Omniglot
    elif args.datasource is "omniglot":
        loss_func = F.cross_entropy
        loss_func = torch.nn.CrossEntropyLoss(reduction='none')
    
    optimizer = optim.Adam(model.parameters(), lr=args.beta)

    # sinusiodal
    if args.datasource is "sinusoidal":
        batch_size = 25 
        num_step = 1
        num_examples = 5
        max_iter = 70000
    # omniglot iteration
    elif args.datasource is "omniglot":
        max_iter = 40000
        num_classes = 20    # 20-way
        num_examples = 1    # 1-shot
        batch_size = 16     # for 20-way 1-shot
        alpha = .1          # for 20-way 1-shot
        num_step = 5        # for 20-way 1-shot

    num_classes = 5     # 5-way
    num_examples = 1    # 1-shot
    batch_size = 32#8#16#32     # for 5-way 1-shot
    alpha = .4          # for 5-way 1-shot
    num_step = 1        # for 5-way 1-shot

    validation = False
    for i in range(max_iter):
        # for sinusoidal
        xs = np.zeros((batch_size, num_examples))
        ys = np.zeros((batch_size, num_examples))

        # for omniglot
        if validation:
            input_characters = [random.sample(metaval_character_folders, num_classes) for i in range(batch_size)]     # (N, C)
        else:
            input_characters = [random.sample(metatrain_character_folders, num_classes) for i in range(batch_size)]     # (N, C)
        imagePath_label = [[(os.path.join(character, image), j) \
                    for j, character in enumerate(input_characters[i]) \
                    for image in random.sample(os.listdir(character), num_examples*2)] \
                for i in range(batch_size)]                                                                         # (N, C*K*2)
        
        imagePath_label = [[[i[j*2] for j in p]+[i[j*2+1] for j in p] 
                    for p in [np.random.permutation(num_classes)]][0]
                for i in imagePath_label]

        imagePaths = [[imagePath_label[i][j][0] for j in range(num_classes*num_examples*2)]
            for i in range(batch_size)]
        labels = [[imagePath_label[i][j][1] for j in range(num_classes*num_examples*2)]
            for i in range(batch_size)]
        
        images = np.array([[list(Image.open(imagePaths[i][j]).rotate(random.choice(rotations)).getdata())                            # (N * C * K, 28*28)
                for j in range(num_classes*num_examples*2)]
            for i in range(batch_size)]) / 255.
        images = 1-images
        labels = np.array(labels)
        input_image_shape = (-1, 1, 28, 28)

        input = torch.tensor(images[:, :num_classes*num_examples]).float().to(device)
        inputb = torch.tensor(images[:, num_classes*num_examples:]).float().to(device)

        target = torch.tensor(labels[:, :num_classes*num_examples]).long().to(device)
        targetb = torch.tensor(labels[:, num_classes*num_examples:]).long().to(device)
        
        weight0 = {}
        for name, pram in model.named_parameters():
           weight0[name] = pram 
        fast_net._copy(weight0)

        predicts = []
        losses = []
        meta_predicts = []
        meta_losses = []
        for j in range(batch_size):
            logits = fast_net(input[j].reshape(input_image_shape))
            loss_ = loss_func(logits, target[j]) / num_examples
            loss = torch.mean(loss_, 0, True)
            
            losses.append(loss)
            predict = torch.softmax(logits, dim=1)
            predicts.append(predict.reshape(-1, num_examples*num_classes, num_classes))

            grads = torch.autograd.grad(losses[-1], fast_net.parameters(), create_graph=True) 
            weight_ = OrderedDict([ (name, weight0[name] - alpha*grad)
                        for name, grad in zip(weight0, grads) ])
            meta_logits = fast_net.forward(inputb[j].reshape(input_image_shape), weight_)
            meta_loss_ = loss_func(meta_logits, targetb[j]) / num_examples
            meta_loss = torch.mean(meta_loss_, 0, True)

            meta_losses.append(meta_loss)
            meta_predict = torch.softmax(meta_logits, dim=1)
            meta_predicts.append(meta_predict.reshape(-1, num_examples*num_classes, num_classes))
        loss = torch.mean(torch.cat(losses))
        meta_loss = torch.mean(torch.cat(meta_losses))
        predicts = torch.cat(predicts)
        meta_predicts = torch.cat(meta_predicts)

        if classification == True:
            accuracy = (torch.argmax(predicts, dim=1) == target).float().mean()
            meta_accuracy = (torch.argmax(meta_predicts, dim=1) == targetb).float().mean()
        
        with open("logs/%s/log.txt"%args.log_folder, "a") as log_file:
            log_file.write("%5d\t%10.4f\n"%(i, meta_loss.item()))

        if i % 100 == 0:
            #print("%4d, loss=%.4f"%(i, meta_loss))
            if classification:
                print("%4d, preloss=%.4f \t postloss=%.4f"%(i, loss, meta_loss))
                print("%4d, preaccuracy=%.4f \t postaccuracy=%.4f"%(i, accuracy, meta_accuracy))
            else:
                print("%4d, preloss=%.4f \t postloss=%.4f"%(i, loss, meta_loss))
            torch.save(model, "logs/%s/%05d.pt"%(args.log_folder, i))
        optimizer.zero_grad()

        if validation:
            validation = False
        if i % 500 == 0:
            validation = True
        else:
            meta_loss.backward()
#        for name, param in model.named_parameters():
#            print(param.grad)
            optimizer.step()
        
        # for validation


if __name__ == '__main__':
    main()
